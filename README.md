# ğŸ“„ RAG Q&A with PDF & Chat History
![RAG Banner](image.png)

> Transform your PDFs into an interactive, context-aware Q&A experience! Ask questions about your PDFs and get concise, accurate answers while retaining the entire conversation context. Perfect for research, documentation, or learning from large PDF files.  

---

## ğŸš€ Features

- **ğŸ“‚ Multiple PDF Uploads:** Upload one or multiple PDFs simultaneously.
- **ğŸ” Intelligent Question-Answering:** Ask questions about the content of your PDFs and get answers derived directly from the document.
- **ğŸ§  Context-Aware Responses:** The system remembers previous questions and answers, allowing follow-up questions that reference past interactions.
- **ğŸ’¬ Chat History Management:** Each session tracks conversation history for continuity across multiple questions.
- **âš¡ HuggingFace Embeddings:** Semantic understanding of documents using `all-MiniLM-L6-v2` embeddings.
- **ğŸ“š Recursive Document Chunking:** Large PDFs are split intelligently to ensure optimal retrieval and context matching.
- **ğŸ’¾ Vector Database:** Chroma DB stores document embeddings for fast and accurate semantic search.
- **ğŸ”‘ Groq LLM Integration:** Uses Groqâ€™s `Gemma2-9B-IT` model for powerful, concise responses.
- **ğŸ–¥ï¸ Interactive Web App:** Built with Streamlit for a smooth, user-friendly interface.
- **ğŸ”’ Secure API Access:** Groq API key entry ensures private and secure usage.
- **ğŸ“ Standalone Question Reformulation:** Converts follow-up questions into independent queries for more precise retrieval.
- **ğŸ› ï¸ Lightweight & Extensible:** Modular code for easy updates or integration with other LLMs.

---

## ğŸ–¼ï¸ How It Works

1. **Upload PDFs:** Drag and drop one or more PDFs into the uploader.  
2. **PDF Loading & Splitting:** Each PDF is loaded and split into smaller chunks for better semantic understanding.  
3. **Embeddings:** Each chunk is converted into vector embeddings using HuggingFaceâ€™s `all-MiniLM-L6-v2`.  
4. **Vector Storage:** Embeddings are stored in Chroma DB for efficient retrieval.  
5. **Question Input:** Type your question into the input box.  
6. **Context-Aware Retrieval:** The system searches for relevant chunks and reformulates your question if it depends on previous chat history.  
7. **Answer Generation:** Groq LLM generates concise answers (max 3 sentences) using the retrieved context.  
8. **Chat History:** All questions and answers are stored per session for follow-up queries.

---

## ğŸ› ï¸ Installation & Setup

1. **Clone this repository:**
   ```bash
   git clone https://github.com/musfiqurrabeg/RAG-QA-Chatbot-with-PDF-and-History.git
   cd RAG-QA-Chatbot-with-PDF-and-History
   ```
2. **Create a .env file and add your HuggingFace token:**
```bash
   HF_TOKEN=your_huggingface_token
```

3. **Run the Streamlit app:**
```bash
   streamlit run app.py
```

4. **Enter your Groq API key and session ID in the app to start asking questions.**

## ğŸ”‘ Usage

 - Upload PDFs â†’ Enter your API key â†’ Ask your questions â†’ Get context-aware answers.

 - You can ask follow-up questions referencing earlier questions â€” the system remembers your session!

 - Ideal for summarizing, studying, or quickly finding information from large PDFs.

 
## ğŸ§© Technical Details

 - Vector Store: Chroma DB

 - Embeddings: HuggingFace all-MiniLM-L6-v2

 - LLM: Groq Gemma2-9B-IT

 - Framework: Streamlit

 - Document Loader: PyPDFLoader

 - Text Splitter: RecursiveCharacterTextSplitter

 - History Management: LangChain ChatMessageHistory

- RAG Architecture: Retrieval-Augmented Generation with history-aware retrieval


## ğŸ”¥ Why This Project is Unique

Unlike traditional PDF readers, this app answers your questions intelligently and remembers your conversation. Itâ€™s not just a search toolâ€”itâ€™s like having a personal assistant for your PDFs!
